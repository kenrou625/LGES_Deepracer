# LGES_Deepracer
LGES's Deepracer challenge project

AWS에서  지원하는 DeepRacer에서는 차량 자율주행을 위한 보상함수를 넣어서 시뮬레이션을 해보는 작업을 지원한다.
DeepRacer를 통해 정해진 일정한 트랙을 최단시간안에 주행하는 보상함수를 만들어 본다.
AWS에서 Policy gradient Algorithm을 지원하고 있으며(내부 network구조를 명확히 확인하기는 어려움) 여기에서 보상함수만 수정해서 training을 시킬 수 있다.


# Reward function and  Training

### 테스트 모델 생성
각 성능에 대한 reward를 각각 계산하여, 마지막에 비중별로 합치는 방법을 이용해보았다.
total reward = reward_lane + 4.0 * reward_avoid + 1.0 * reward_heading + 0.5 * reward_speed
그러나, 각 하나나 두개를 조합한 reward로직보다 학습시간만 오래걸리고 각 reward에 대한 효과를 관찰할 수 없었다.
그래서, 이후에는 1~2개의 reward조합으로 모델을 구성하는 방안으로 진행해보았다.
+ 상위 랭커들은 모두 track의 way point정보를 이용해서, 각 코스의 최적의 주행 라인을 따라가도록 학습한것을 파악하고, way point를 이용한 로직도 추가가 필수적이라는 것을 느꼈다.

### 첫번째 모델 생성
첫번째로는 AWS에서 제공하는 Sample reward function을 이용해서 학습을 진행하였다.
Reward :  트랙 중심으로부터 거리가 가까울수록 높은 보상, 바퀴가 모두 트랙내에 존재하면 보상, 핸들기울기가 15도를 넘어가면 보상을 깎음.(직선구간에서 지그재그로 움직이는것을 방지하기위함.)
결과적으로 training percentage completion은 35%정도 나왔고, Evaluating에서는 20%이하로 나왔다. Ranking은 아래와같다.




### 두번째 새로운 모델 생성
두번째로는 트랙의 waypoint들을 이용해서 해당 waypoint의 center를 벗어나지 않도록 학습하는 조건으로 보상함수를 짜서 학습해보았다.
아래사진과 같이 트랙에 waypoint들을 이용해서 커브구간에 대한 제어를 하려고 하였다. 단점은 해당 트랙에 과적합될 수도 있다.
좌회전구간에서 중심에서 왼쪽에 있으면 보상(반대도 똑같이 보상), 직진구간에서 중심에서거리/너비 가 0.4 미만이면 보상
결과적으로는 템플릿과 큰 차이를 보이지 않았다.  비슷한 percentage completion을 보였다.





### 세번째 모델 생성
첫번째 모델과 두번째 모델의 보상함수를 결합해서 사용하였다. 무조건 두개를 결합한다고 좋아지는것은 아니지만, 좀더 나은 결과가 나올 수도 있지 않을까하는 생각에서 진행하였다.
차량의 최대 속도를 줄이고 카메라 각도 갯수도 좀 더 줄여서 모델을 가볍게 만들어서 진행하였다.
보상함수의 학습률은 굉장히 개선되었다. 이유는 모델이 훨씬 가벼워지고 차량 속도를 줄임으로서 차선이탈없이 완주도 가능해진 것으로 보인다.
결과적으로 모델을 복잡하게하고 속도를 올리려면 더 많은 학습시간이 필요할 것 같다.(한시간 학습에 사용되는 컴퓨팅에 4달러가 필요하다.) 
AWS DeepRacer에서 진행하는 리그의 경우 최단시간에 트랙을 도는것을 목표로 하고있기 때문에 정확하게 트랙을 벗어나지 않고 완벽하게 주행을 하는 것과 약간의 괴리가 존재한다. 
현재 모델의경우, track completion이 다른 모델 대비 훨씬 향상되었음에도 불구하고, 리그 등수는 큰 변화가 없는 것만 봐도 알 수 있다.





### 네번째 모델 생성
세번째 모델과 같은 모델이지만 차량의 Maximum속도를 올리고 카메라의 각도 갯수를 더 늘려 좀 더 복잡한 모델로 만들어 보았다.
결과는 track completion은 떨어지지만 확실히 이전보다 리그에서 주행에 걸린 시간은 단축할 수 있었다. 
Completion을 높이면서 시간을 줄이는 방법에 대해 좀 더 고민이 필요해 보인다.






### 다섯번째 모델 생성
waypoint를 이용해서 직선거리에서 핸들링각도에 따라 보상함수를 조절하고, 속도를 올려주도록 코드를 수정하였다.
학습이 진행됨에다라 학습률이 우상향하는 모습을 보이지는 않았으나, 해당 모델로 test했을때 가장 좋은 성적을 만듬.
